
\subsubsection{Runtime and Tuple Delay} % Commenting on the results of n,K,L,P experiments. 

First of all, the runtime was expectedly linear to the data stream length, which helps us draw the conclusion that the algorithm probably not will be neither more or less efficient for infinitely long data streams. \\
As it was to be expected, the average delay of the tuples increases with an increase of $k$, as shown in fig \ref{fig:kvalues}. As the window for each $QID$ needs more tuples to reach K-anonymity and thereby released, the result is longer relay for the tuples already in that window. In comparison, the runtime evaluation of different values of $k$ in fig. \ref{fig:runtimekvals} showed that despite these delays, the algorithm in itself still finishes with constant efficiency independent of the size of $k$. \\
However, it is hard to draw conclusions regarding the tuple delay variation depending on $l$. In our experiments, the runtime appears to be constant for any $1 \leq l \leq 10 $, where 10 is the number of different sensitive classes in our data and hence the largest possible diversity among any group of tuples. Since this is a relatively small value, for example in comparison to $k$, it can not be excluded that the runtime may vary for larger values of $l$. \\
% Probably remove but not sure: This result differs from most previous approaches which use k-anonymity on batches of data. Assumably, this is due to that large batches of data and faster, that is why other researchers find decreasing latencies for larger k. for us it is the other way around. however anonymizing large batches, has the shortcoming that there might be outliers and therefore the generalization needs to be strong in order to get all tuples on one level, which leads to a large information loss
As seen in fig. \ref{fig:runtimepvals} and fig. \ref{fig:pvalues}, it could not be found that the method benefited from parallelization in terms of either total efficiency or tuple delay. A possible explanation could be that in the current setup, the overhead of running multiple processes is higher than the possible benefits of the parallelization, thus preventing real advantage. It is possible that parallelization will be more beneficial as the number of possible $QID$ generalizations increase. 

\subsubsection{Stuck Tuples} % Commenting on the stuck tuples experiments and our approach in general 
The experiments could show that the number of stuck tuples remained constant for any tested data stream length. This is an expected outcome. As mentioned in section \ref{sect:algorithm} the amount of stuck tuples has an upper bound induced by the number of unique possible combinations of the generalized quasi-identifier. Furthermore, the distribution of fig. \ref{fig:stuckdist} show that the tuples still in the pipeline was predominantly tuples which were inserted during the last 10\% of the data stream lifetime. While expected, these results verify that the logic behind the approach of keying tuples by $QID$ is actually working in practice. However, it must be pointed out that the upper bound of stuck tuples will become significantly bigger with a more complex $QID$, and that depending on data distribution a significant amount of tuples may still become stuck indefinitely in some implementations. 

\subsubsection{The keying by $QID$-approach} % Suggesting future experiments (especially regarding our approach)
As mentioned in \ref{sect:algorithm}, many current approaches of applying \textit{k}-anonymity on streamed data induce an information loss that increases linearly with \textit{k}. Through these evaluations, we could show that the approach of keying by $QID$ with a weak, static generalization introduces a comparably small information loss, at the price of an introduced tuple delay. While the mentioned approach may reduce information loss, there still remains tuples in the pipeline considered as stuck or very delayed in the current form of the approach. As a possible solution, we would like to suggest the possibility to continuously collecting, applying \textit{k}-anonymizing and releasing tuples which are considered as stuck according to a certain constraint. Such a constraint could be either expressed as time delay or order delay. As the percentage of stuck tuples showed to be very low, the potential information loss in total remains low. 

% Notes MM:
% I guess we could reason in the report that through our method, we can apply % a weak generalization, thus reducing information loss. Then, the stuck % % % tuples (in our case ca 15% of the total number of tuples) will be 
% anonymized with the algorithm.
